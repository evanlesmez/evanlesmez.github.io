[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts üêíüêßüêç",
    "section": "",
    "text": "Devlog Day 12\n\n\n\n\n\n\n\nelectronics\n\n\n\n\nEndeavors in a puting together a portable mini PC workstation\n\n\n\n\n\n\nAug 14, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 12\n\n\n\n\n\n\n\ngamedev\n\n\ngodot\n\n\nvim\n\n\n\n\nVim tab layouts\n\n\n\n\n\n\nAug 13, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 11\n\n\n\n\n\n\n\ngamedev\n\n\ngodot\n\n\ntools\n\n\n\n\nGodot4 Health pickups\n\n\n\n\n\n\nAug 12, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 10\n\n\n\n\n\n\n\nvim\n\n\ngamedev\n\n\ngodot\n\n\n\n\nVim window management and Godot Area2D physics\n\n\n\n\n\n\nAug 11, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 9\n\n\n\n\n\n\n\nvim\n\n\ngamedev\n\n\ngodot\n\n\n\n\nCamera work in Godot and Vim buffers\n\n\n\n\n\n\nAug 10, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 8\n\n\n\n\n\n\n\nvim\n\n\ngamedev\n\n\ngodot\n\n\n\n\nVim buffers + a steering algorithm Godot\n\n\n\n\n\n\nAug 7, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 7\n\n\n\n\n\n\n\nvim\n\n\ngamedev\n\n\ntech\n\n\n\n\nVim cmd line window & sh + smoothing movement in Godot\n\n\n\n\n\n\nAug 6, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 6\n\n\n\n\n\n\n\nvim\n\n\ngamedev\n\n\n\n\nVim practice and player movement with normalized vectors in Godot\n\n\n\n\n\n\nAug 5, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 5\n\n\n\n\n\n\n\nvim\n\n\ngamedev\n\n\ntheprimagean\n\n\n\n\nVim and moving sprites in Godot\n\n\n\n\n\n\nAug 4, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 4\n\n\n\n\n\n\n\ngamedev\n\n\nvideo games\n\n\ntech\n\n\n\n\nGDScript\n\n\n\n\n\n\nAug 3, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 3\n\n\n\n\n\n\n\ngamedev\n\n\nvideo games\n\n\ntech\n\n\n\n\nGDScript\n\n\n\n\n\n\nJul 31, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 2\n\n\n\n\n\n\n\ngamedev\n\n\nvideo games\n\n\ntech\n\n\n\n\nTour of Godot\n\n\n\n\n\n\nJul 31, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nDevlog Day 1\n\n\n\n\n\n\n\ngamedev\n\n\nvideo games\n\n\ntech\n\n\n\n\nGetting started with 2D Game Development in Godot\n\n\n\n\n\n\nJul 30, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nAlgorithms inherit the prejudices of their creators and society.\n\n\n\n\n\n\n\nethics\n\n\nai\n\n\ntech\n\n\n\n\nMy notes and thoughts on Deep Learning for Coders: Chapter 3 data ethics\n\n\n\n\n\n\nOct 17, 2023\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nFirst Steps: AI for Animal Advocacy with Fastai üêá\n\n\n\n\n\n\n\nblog\n\n\nfastai\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nEvan Lesmez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03-devlog-day2/index.html",
    "href": "posts/03-devlog-day2/index.html",
    "title": "Devlog Day 2",
    "section": "",
    "text": "Today I completed the tour of Godot game made by the GDQuest creators.\nLearned about TileMap nodes to create repeating visual 2D blocks used in old games like Zelda.\nUsed ‚ÄúTerrains‚Äù to create instances of the TileMaps faster.\nCollision shapes like invisible walls can be represented by shapes in the scene editor to convey that a space is off limits.\nRepeated using a signal to hook up a player node to a UI health bar when an enemy did damage.\nNote:\nBackgrounds add a lot to the feel of a game as does looting a chest.\nI asked a question in the last lesson about any gamedev podcast recommendations.\nNot exactly gamedev, but the creator of the course recommended theprimegean so I watched a few videos today about Vim and made me want to get back on switching to neovim as my editor.\nCurrently I use vim mode wherever possible such as VSCode, Jupyter Lab, Obsidian ‚Ä¶\nI did setup neovim with some Lua plugins but was still too slow compared to working in VS Code for basic things like creating a new file, copying files, searching through all the files for a class‚Ä¶\nBut might be time to start trying again.\nFound some Godot 4 Vim binding plugins for GDScript which will be helpful.\nTomorrow I will be moving onto the next lesson on GDScript."
  },
  {
    "objectID": "posts/08-devlog-day7/index.html",
    "href": "posts/08-devlog-day7/index.html",
    "title": "Devlog Day 7",
    "section": "",
    "text": "I continued working through the Pracitval Vim book today.\nI learned about the cmd line window which is different from the command-line mode.\nAccidentally I have ended up there numerous time by accidentally typing q: rather than :q to exit a file.\nPreviously, it had nover occurred to me to examine what was was going on and instead I would repeat :q twice to exit the mysterious window I was trapped in and then to exit the file as intended.\nNow, I realize that this window allows you to see the history of command you have typed into the command mode as a buffer.\nYou may navigate the history as you like and even manipulate it to run new commands.\nFor example, you could find a command you had split into two like lint file and test file.\nIf you hover over lint file in cmd line window, and go to the end with A and insert a pipe char |, you can then enter J to move the line below to the current line which should result in lint file | test file.\nThen  aka ‚ÄúEnter‚Äù to run your now single line command.\nI also learned you may enter a enw shell session by typing :shell in the command mode but more interesting is that you can background Vim or any shell process with Ctrl+Z.\nThen you may list jobs with jobs.\nIf you wish to reattach a job, just foreground it with fg.\nDefinetly going to use this in the future.\nOther random notes:\n:ls list the open vim buffers while :!ls does what any UNIX user would expect.\nwrite !sh is a neat quick way to run each command in the current buffer in shell environment.\nread instead puts command output into the buffer.\n% refers to the current filename of the buffer.\n! is reffered to as ‚Äúbang‚Äù for executing shell commands.\nOn the Godot front:\n\nI completed adding a boost effect to the moving ship by hitting spacebar.\nThat was pretty simple as it only required increasing the speed to multiply the direction velocity vector by.\nMore interesting was the method for smoothing movement.\nAs shown in the gif above, the method to determine the desired velocity vector was to multiply the speed scalar by the direction vector based on the current movement inputs (including boost).\nSubtracting the current velocity from the desired velocity vector gives us the steering vector or the vector that points in the direction the ship should turn to correct course smoothly from its current state.\nThen we introduce another scalar that we can call steering_amplifier between 0 to 1 (but really above that because we multiply by the delta which is below 1) that is used to determine the lagginess feel of the controls.\nA high value would make the controls not lag at all but also look jittery.\nA low value would feel laggy for movevent of the ship to even stop moving after releasing the controls, however can‚Äôt argue it not smoother than the higher steering values."
  },
  {
    "objectID": "posts/07-devlog-day6/index.html",
    "href": "posts/07-devlog-day6/index.html",
    "title": "Devlog Day 6",
    "section": "",
    "text": "I practiced some Vim today in the same Practical Vim book.\nLearned that Vim writes history to vim info file that persists across machine boots.\nThe history is not only execute commands but also search patterns and I am sure other commands.\nIn execute command mode Ctrl-d.\n* in normal mode searches for the hovered word.\n&lt;C-r&gt;&lt;C-w&gt; in execute mode copies the current word in as pattern for things like substitutions.\nIn Godot, I practiced making a space ship move with player inputs.\nThe most important parts were adding the controls in the Project settings.\nThe direction is stored as a 2DVector.\nThe Input object has an axis method that accepts two input arguments like ‚Äúmove_down‚Äù or ‚Äúmove_up‚Äù to map values between -1 and 1 to.\ndirection is updated to match these values and then scaled by whatever speed is desired.\nLastly, the direction vector must be normalized if the length is above 1.0 which happens when moving diagonally which would make the ship move 1.4 times faster #pythagoreantheorum."
  },
  {
    "objectID": "posts/02-devlog-day1/index.html",
    "href": "posts/02-devlog-day1/index.html",
    "title": "Devlog Day 1",
    "section": "",
    "text": "Disclaimer: Today was not actually my first day of game development nor Godot.\nI actually went to nerdy chess and ‚Äúcode‚Äù camp back in elementary school and built a simple 2d shooter game.\nA several years ago I tinkered with Unity for about a month but lost interest as work at the startup I was with picked up a lot.\nTwo years ago I discovered Godot and went through the getting started docs.\nI built a simple 2D dodge the creeps game.\n\n\n\n\nYesterday I rebuilt it following along with a GDQuest tutorial and that brings us to today.\nI am starting the Learn 2D Gamedev Godot 4 course.\nWhy am I getting back into gamedev and why am I writing this down?\nAs to why gamedev:\nI have always loved games since I was a kid like Super Smash Bros Melee, Sly Cooper, FIFA, Uncharted 2, CoD Modern Warfare 2, and League of Legends.\nI always thought it would be fun to try making one of the several game ideas I have jotted down in my notebooks over the years.\nI now know how to code decently well.\nI think games are a valuable way to spread ideas, especially on issues I care about like veganism, animal activism, and conservation.\nWhy am I writing this down?\nMostly to keep myself accountable.\nAs echoed in both the fastai and GDQuest curriculum, social pressure is inevitablely motivating for every single person wether we admit it or not.\nAs I don‚Äôt have a group of study partners nearby to work on either of those, I am resorting to posting a daily blog that I will share on social media.\nI‚Äôll try to keep these short and to the point in 250ish words.\nWhat I learned so far on gamdev and Godot:\n* pixel art isn‚Äôt as easy as it looks\n* vector art is great to start with for beginners making their earliest games\n* signals are a convenient design pattern to broadcast events from different game objects to each other to determine what should happen next\n* rendering animations for some basic sprites is way simpler than I thought it would be (at least in Godot)\n* music and sounds add a lot to how a game feels\nFor tomorrow I want to continue with the course and hopefully have finished answering the fastai end of chapter questions I have put off for a while."
  },
  {
    "objectID": "posts/00-first-steps-fastai/index.html",
    "href": "posts/00-first-steps-fastai/index.html",
    "title": "First Steps: AI for Animal Advocacy with Fastai üêá",
    "section": "",
    "text": "Code\nfrom PIL import Image\n\nsmol_forest_guardian = Image.open(\n    \"./DALL¬∑E-digital_art_cute_solarpunk_forest_guardian_robot.png\"\n)\ndisplay(\n    smol_forest_guardian.resize((400, 400)),\n    \"Solar punk forest guardian source: DALLE¬∑2\",\n)\n\n\n\n\n\n'Solar punk forest guardian source: DALLE¬∑2'\nThis charming little forest robot was created using OpenAI‚Äôs DALL¬∑E 2 model, based on my prompt: ‚ÄúDigital art cute solarpunk forest guardian robot‚Äù.\nThis image represents an idea I‚Äôve been interested in for a long time. I‚Äôm not certain where it all started, but I think it goes back to my childhood. That‚Äôs when my aunt introduced me to my first Miyazaki movies: ‚ÄòMy Neighbor Totoro‚Äô, ‚ÄòSpirited Away‚Äô, and ‚ÄòCastle in the Sky‚Äô.\nCode\nbig_guardian = Image.open(\"./castle_in_sky_guardian.jpg\")\nratio = 0.33\nnew_dimens = (round(big_guardian.width * ratio), round(big_guardian.height * ratio))\ndisplay(\n    big_guardian.resize(new_dimens),\n    \"Castle in the Sky Guardian source: https://characterdesignreferences.com/art-of-animation-9/art-of-castle-in-the-sky\",\n)\n\n\n\n\n\n'Castle in the Sky Guardian source: https://characterdesignreferences.com/art-of-animation-9/art-of-castle-in-the-sky'\nMost Hayao Miyazaki fans, I believe, can relate to the sense of awe they feel when watching his films. He has a unique ability to instill a profound respect for nature, capturing its simple beauty and serving as a daily reminder of how much we often take it for granted. His films advocate for the protection of nature from human exploitation and emphasize the importance of reconnecting with the world around us. The Laputian robot from ‚ÄòCastle in the Sky‚Äô, depicted above, stands as a compelling example of this message.\nA few years ago, perhaps inspired by Miyazaki‚Äôs works, I realized my mission: to develop technology that champions the rights of non-human animals and safeguards our shared ecosystems. I envision a future where AI not only respects nature more deeply than humans currently do, but also unravels its secrets that remain undiscovered.\nTo take steps towards this goal, I am embarking on a journey to learn about deep learning, one of the most promising fields within AI. This blog will serve as a record of my progress, where I‚Äôll document my practice and share related ideas, lessons, and questions that arise along the way."
  },
  {
    "objectID": "posts/00-first-steps-fastai/index.html#fast.ai",
    "href": "posts/00-first-steps-fastai/index.html#fast.ai",
    "title": "First Steps: AI for Animal Advocacy with Fastai üêá",
    "section": "fast.ai",
    "text": "fast.ai\nFastai is a vibrant community of deep learning enthusiasts, dedicated to making AI accessible to all. I‚Äôm currently going through their ‚ÄòPractical Deep Learning for Coders‚Äô course, which has been fantastic thus far!\nI‚Äôd highly recommend this course to anyone with even a hint of programming experience who‚Äôs curious about AI. This is particularly true if you‚Äôre in an industry where AI development is still in its infancy - there could be a significant opportunity waiting.\n\nSurprising Discoveries (so far)\nI was astounded by the speed at which I could train and deploy my first model - all within a few weeks of learning.\nTransfer learning is a technique that involves taking a pre-trained model with expert-determined weights and fine-tuning it with your specific data.\nThis strategy allows you to quickly implement a functioning model, without the need to start from scratch each time. As an example, I trained a simple greenhouse/hydroponic plant health classifier using a pre-trained image classifier model based on the ResNet18 architecture. This was a problem a previous company I worked at was trying to solve, so I thought it would be a fun challenge to undertake myself.\nMy trained model is now deployed on this ü§ó Hugging Face space.\nHere‚Äôs a fun fact ü§ì: A GPU isn‚Äôt necessarily required for a deployed model.\n\n\n\nScreenshot of the plant-doc ü§ó space\n\n\nBelow are some snippets from the training of the model, conducted in a Google Colab notebook.\n\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\n\n# ... create a labeled image datablock and visualize\n\nhydro_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128),\n)\ndls = hydro_dblock.dataloaders(path)\ndls.valid.show_batch(max_n=8, nrows=2, figsize=(8, 5))\n\n\n\n\nLabeled datablock\n\n\n\n# ... use a pretrained learner and fine tune\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\nEpoch table\n\n\n\n\nOvercoming Initial Fears in Deep Learning\nBefore diving into the world of deep learning, I was somewhat daunted by the complexity I feared training and deploying a model would entail. I‚Äôm neither a math whiz nor a master coder, but I found many of the initial concepts far more intuitive than I‚Äôd anticipated.\nFor instance, the practice of maintaining a separate training set of data from a validation set (and a test set) seemed quite logical. The training set provides the model with a foundational understanding of correct answers, like labeled images. The validation set then serves as a quiz for your model, checking its comprehension of the patterns it has learned. In the context of an image classifier, the model must guess which label best matches a given image from the validation set, and then evaluate the confidence level of its correctness or error. This process facilitates the model‚Äôs improvement with each ‚Äúepoch‚Äù or training cycle. Additionally, a completely separate test set, kept hidden from the model, can be used by humans to assess the model‚Äôs performance after training is completed.\n\n\n\nSimplified model training through test diagram\n\n\nSeparating a robust validation set (and test set) helps to prevent overfitting the model to images present only in the training set. Overfitting can render models unreliable for new images encountered outside the ‚Äúlab‚Äù setting.\nFor instance, if you‚Äôre building a cat breed classifier and include numerous images of the same orange cat perched on its cat tower in both the training and validation sets, the model might overfit for that particular scenario.\nAnother concept I found intuitive and valuable is the confusion matrix. The confusion matrix helps us visualize which labels the model was ‚Äúconfused‚Äù by and predicted incorrectly during training. For example, as shown below, the model predicted that a few plants were healthy when they were actually wilted, and vice versa.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\nConfusion matrix\n\n\nWe can also plot the top mistakes to visualize the images where the model made incorrect predictions and evaluate the model‚Äôs confidence in its decisions. Being confidently wrong is problematic, but so is being correct with low confidence. Both scenarios suggest areas where the model can improve.\nIn the first case, the model may have ‚Äòlearned‚Äô incorrect patterns from the training data, leading to high confidence in wrong predictions. In the second case, the model‚Äôs lack of confidence, even when correct, could suggest that it‚Äôs struggling to find clear patterns in the data. These are valuable insights that can guide us in improving the model‚Äôs performance.\n\ninterp.plot_top_losses(5)\n\n\n\n\nPlot of top losses\n\n\n\n\nOpportunities for Deep Learning in the Animal and Vegan Advocacy Movement\nThe Animal and Vegan Advocacy (AVA) movement has a multitude of opportunities to leverage deep learning. Just to name a few:\n\nMonitoring wildlife habitats\nIdentifying illegal deforestation\nFlagging illegal fishing vessels\nBuilding vegan education chatbots\nEnforcing farmed animal welfare standards\n\nSome of these areas are already seeing progress. For example, check out this AI4Animals animal welfare issue camera monitoring system developed in the Netherlands.\nOne of the most intriguing projects I‚Äôve come across in this field is the Earth Species Project. Their goal is to decode non-human communication using Natural Language Processing. The potential to understand the ‚Äòsecret languages‚Äô animals use could undoubtedly foster more compassion.\n\n\nObstacles Faced by the Movement\nNon-profit organizations, particularly those advocating for animal rights, often face resource constraints that aren‚Äôt an issue for for-profit industries. Even within the landscape of animal non-profits, farmed animal activism receives only a fraction of the donations that shelters do.\n\n\n\nChart of animals impacted and donations received for animal charities\n\n\nMoreover, non-profits frequently lag behind in technology adoption, making it challenging not only to attract talent like Machine Learning engineers, but also to pursue deep learning-enabled projects that have the potential to make a significant impact.\nLarge animal agriculture enterprises, armed with extensive resources, are using AI to enhance their efficiency, often without considering animal welfare or ecosystem health. Historically, technology has been used to exploit our environment, damaging natural habitats and harming wildlife. If left unchecked, AI could further this trend.\nWe need to empower compassionate individuals and policymakers to better understand AI. This will ensure its use strikes a healthier balance between technological advancement and nature, rather than exacerbating existing problems.\nThank you for reading, and stay tuned for more posts in the future!\nThis blog was built with Quarto and Jupyter, allowing me to embed fun, interactive code generated blocks like the one below.\nTry hovering over it.\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n\nimport numpy as np\n\n# Helix equation\nt = np.linspace(0, 20, 100)\nx, y, z = np.cos(t), np.sin(t), t\n\nfig = go.Figure(\n    data=[\n        go.Scatter3d(\n            x=x,\n            y=y,\n            z=z,\n            mode=\"markers\",\n            marker=dict(size=12, color=z, colorscale=\"spectral\", opacity=0.8),\n        )\n    ]\n)\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0), width=640, height=640)\nfig.show()"
  },
  {
    "objectID": "posts/12-devlog-day11/index.html",
    "href": "posts/12-devlog-day11/index.html",
    "title": "Devlog Day 11",
    "section": "",
    "text": "Only had time for some gamedev today.\nI learned a lot about memcached though which is an earlier, kind of worse version of redis.\nUseful for caching DB queries and probably other expensive data fetching services.\nAlso in turn learned about telnet which is an earlier, definetly worse version of SSH.\nWorse because it has no encryption and the protocol for communicating with the server is not as intuitive.\nUseful for LAN servers though, especially old ones that don‚Äôt support SSH.\nCompletely tangential, I got more into tiling window management and discovered komorebi for Windows.\nOn Linux distributions i3 and other programs are available but it is nice to have an option for Windows so my desktop can look like this."
  },
  {
    "objectID": "posts/12-devlog-day11/index.html#health-pickups",
    "href": "posts/12-devlog-day11/index.html#health-pickups",
    "title": "Devlog Day 11",
    "section": "Health pickups",
    "text": "Health pickups\n\nIn the gif, you see the ship driving around and picking up the health packs.\nIn the remote scene tree on the left, the nodes of the health packs are removed once picked up.\nThis is from the signals emited from the Area2D nodes that have a callback to queue_free.\nA common node pattern seems to be:\nArea2D\n|_ Sprite2D\n  - texture\n  |_ GPUParticles or ther Sprite2Ds or Line2Ds\n|_ CollisionShape2D\n  - shape\nThank for reading."
  },
  {
    "objectID": "posts/10-devlog-day9/index.html",
    "href": "posts/10-devlog-day9/index.html",
    "title": "Devlog Day 9",
    "section": "",
    "text": "I missed a few days ¬Ø\\(„ÉÑ)/¬Ø\nOh well.\nI DIDN‚ÄôT MISS TODAY.\nOn the Godot front, I completed adding some extra art and camera work in line with the GQQuest tutorial.\nHere is what it ended up looking like:\nThe Camera2D Node was simple, just needed to be plopped as a child of a node (the Ship node in this case) to cause the screen to follow it.\nThe thruster rendering code though, not so simple.\nI don‚Äôt understand a lot of what was going on with it.\nAlso refreshed on how vector.normalized() works under the hood to ensure pointing diagonally is the same speed as horizontal/vertical.\nThe math stems from the Pythagorean theorem for right angle triangles.\nThe lengths of the two sides squared, summed, and square rooted is equal to the length of the hypotenuse.\nA given vector, like the direction the player wishes to travel at, has length equal to the hypotenuse of the triangle formed by the x and y arguments of the vector.\nOne side could be 0,0 o x,0, the next side is the x,0 to x,y, and the hypotenuse is 0,0 to x,y.\nSo to normalize a vector, take the length aka the hypotenuse of the triangle formed by the vector, and divide the x and y by it, essentially turning both into a fraction of the the total length of the vector.\nThat made sense to me at least."
  },
  {
    "objectID": "posts/10-devlog-day9/index.html#vim-buffers-arent-scary",
    "href": "posts/10-devlog-day9/index.html#vim-buffers-arent-scary",
    "title": "Devlog Day 9",
    "section": "Vim buffers aren‚Äôt scary",
    "text": "Vim buffers aren‚Äôt scary\nEarly on when starting to program I heard the word ‚Äòbuffer‚Äô thrown around a lot and I wasn‚Äôt sure what it exactly meant.\nI imagined kind of like the buffers on the side of roads providing safety by taking up space, the concept of buffers in computer science might be some space in memory that is occupied to make an application more secure or reliable (kind of like roadway safety) in someway.\nWell in Vim, that idea is close but it is more like the buffers are in memory representations of the files you open to manipulate.\nSo not so much reliability or security, and more like core to how the program functions.\nSome notes on buffer commands:\nBuffers can be opened via glob wildcars like ‚Äú*‚Äù or even ‚Äú**‚Äù for directory recursive file matching.\nCommon patterns would be *.{extension}\nYou can list multiple glob patterns like vim **/*.js **/*.go\nargs are seperate from bufers opened.\nIt‚Äôs kind of like keeping program windows open in different desktops.\nDesktop1 with a few programs would be the args and all of the programs opened all together are the buffers.\n:ls would show every program while args would show desktop1‚Äôs programs.\nargs {arglist} clears the current argslist for the new one.\nUse glob patterns or filenames however you please for the arglist.\nHidden buffers had gotten me before.\nWhen making a change to a buffer and trying to quit or move to a different buffer, vim will warn you that you have changes that are not written yet.\nKinda nice when you think about it, kind frustrating when you haven‚Äôt thought about it and are stuck in a vim window you don‚Äôt know how to exit.\nMost of the time this is helpful reminder but in the case of :argdo when you want to execute commands on all of the buffers in arglist, if the buffers are manipulated the command will be interrupted on each buffer.\nThis behavior can be disabled with the hidden setting flag.\nThank for reading."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Evan Lesmez",
    "section": "",
    "text": "This a blog about about my learning journey delving into topics like deep learning/AI, game development, other programming domains, and random ideas. I am a software engineer. My main passions are veganism and animal activism, games, and tech."
  },
  {
    "objectID": "posts/11-devlog-day10/index.html",
    "href": "posts/11-devlog-day10/index.html",
    "title": "Devlog Day 10",
    "section": "",
    "text": "Today I learned about splitting and managing windows in Vim.\nI had some experience with this before but it was a good refresher.\nMy problem is that I sometimes get mixed up with tmux cmds and Vim ones.\nIn tmux, Ctrl-B is the beginning of commands.\nIn Vim, Ctrl-W is the beginning of window commands.\n&lt;Ctrl-w&gt;s # split window intro rows\n&lt;Ctrl-w&gt;v # split window vertically into columns\n:sp {file} # split rows loading file into new window\n:vsp {file} # split cols loading file into new window\n\n&lt;Ctrl-w&gt;w or &lt;Ctrl-w&gt; # cycle between windows\n# you can also ctrl-w into any hjkl move\n\n:clo # close active window but use :q its easier or &lt;Ctrl-w&gt;c\n:on[ly] or &lt;Ctrl-w&gt;o # close all other windows except current\n&lt;Ctrl-w&gt;= # equalize the dimensions of all windows\n&lt;Ctrl-w&gt;_ # max the current window height\n&lt;Ctrl-w&gt;| # max the current window width\n\n# Moving windows\n&lt;Ctrl-w&gt;r or R # rotates the windows for viewing\n&lt;Ctrl-w&gt;x # swap neighbor window for current\n&lt;Ctrl-w&gt;[hjkl] # bring window to the top of that direction\nThe moving section came from this vimcast on Vim windows."
  },
  {
    "objectID": "posts/11-devlog-day10/index.html#vim-windows",
    "href": "posts/11-devlog-day10/index.html#vim-windows",
    "title": "Devlog Day 10",
    "section": "",
    "text": "Today I learned about splitting and managing windows in Vim.\nI had some experience with this before but it was a good refresher.\nMy problem is that I sometimes get mixed up with tmux cmds and Vim ones.\nIn tmux, Ctrl-B is the beginning of commands.\nIn Vim, Ctrl-W is the beginning of window commands.\n&lt;Ctrl-w&gt;s # split window intro rows\n&lt;Ctrl-w&gt;v # split window vertically into columns\n:sp {file} # split rows loading file into new window\n:vsp {file} # split cols loading file into new window\n\n&lt;Ctrl-w&gt;w or &lt;Ctrl-w&gt; # cycle between windows\n# you can also ctrl-w into any hjkl move\n\n:clo # close active window but use :q its easier or &lt;Ctrl-w&gt;c\n:on[ly] or &lt;Ctrl-w&gt;o # close all other windows except current\n&lt;Ctrl-w&gt;= # equalize the dimensions of all windows\n&lt;Ctrl-w&gt;_ # max the current window height\n&lt;Ctrl-w&gt;| # max the current window width\n\n# Moving windows\n&lt;Ctrl-w&gt;r or R # rotates the windows for viewing\n&lt;Ctrl-w&gt;x # swap neighbor window for current\n&lt;Ctrl-w&gt;[hjkl] # bring window to the top of that direction\nThe moving section came from this vimcast on Vim windows."
  },
  {
    "objectID": "posts/11-devlog-day10/index.html#godot-area2d-collision-physics",
    "href": "posts/11-devlog-day10/index.html#godot-area2d-collision-physics",
    "title": "Devlog Day 10",
    "section": "Godot Area2D Collision physics",
    "text": "Godot Area2D Collision physics\nSo in previous lessons, I only had tinkered with the Sprite2D node which is for visuals and animations.\nHowever, to get some real physics going between objects, like a collision, Area2D is the node for the job.\nIt needs a CollisionObject2D as a child node which in turn needs a shape to use.\nWith those nodes setup, the Area2D signal area_entered may be connected to respond to when another Area2D node collides with this one.\narea_entered.connect can be used in the attached script instead of using the signal editor.\nIt is common to place it in the _ready() function which is what loads the node in the node tree kind of like __init__ or constructor for a class in OOP.\nThe convention for the callback to connect is to name it _on_area_entered implying it is a private method for this node and that it is connected to that signal name.\nThat pattern will be reused for other signals.\nThe callback for the case of loot being picked up by a player will likely cause the node to disappear, which means we will use queue_free.\nqueue_free means to queue up freeing the memory allocated to the node so it will deleted.\nfunc _ready():\n  area_entered.connect(_on_area_entered)\n  \nfunc _on_area_entered(area_that_entered: Area2D):\n  queue_free()\nThat pattern all together is used in scenes like this:\n\nThank for reading."
  },
  {
    "objectID": "posts/05-devlog-day4/index.html",
    "href": "posts/05-devlog-day4/index.html",
    "title": "Devlog Day 4",
    "section": "",
    "text": "Today I completed the intro to GDSript games and learned a few interesting things:\nThe _process builtin function is the ‚Äúgame loop‚Äù of Godot to place code that needs to trigger every frame.\nIt has a parameter called ‚Äòdelta‚Äô.\nDelta is a time difference since the previous frame.\nReferencing delta in calculations helps the game feel more smooth across various frame rates.\nlerp is a function that returns a weighted average between two values (linear interpolation).\nYou can auto type hint with the := operator like var name := \"gibby\".\nMost importantly, GDScript is basically Python.\nUp next is working on a top-down action game in space."
  },
  {
    "objectID": "posts/01-algorithms-inherit-bias/index.html",
    "href": "posts/01-algorithms-inherit-bias/index.html",
    "title": "Algorithms inherit the prejudices of their creators and society.",
    "section": "",
    "text": "IBM leadership with Adolf Hitler\nIn 1939, under the direction of its president, Thomas Watson, IBM released specialized alphabetizing machines.\nThese machines along with other IBM products played a pivotal role in organizing the deportation of Polish Jews for Nazi Germany.\nThe IBM leadership didn‚Äôt just passively supply; they actively marketed their technology directly to Hitler and his top officials.\nJust a few years prior, IBM‚Äôs CEO, Tom Watson Sr., had even been honored with a ‚ÄúService to the Reich‚Äù medal.\nThe company implemented a punch card system to track the method of execution and the ethnicity of each victim.\nAnd to ensure these machines ran efficiently, IBM staff were often present on-site at concentration camps for operation and maintenance.\nIBM thrived financially and technologically, but at what cost to humanity?\nIBM concentration camp punch-card - Source\nWe are all aware of the horrors of the holocaust and can agree IBM‚Äôs involvement is an egregious example of focusing on technology without care to its impact on society.\nNowadays we are fortunate enough to be in, for the most part, more peaceful times.\nHowever the same ethical questions still arise with development of new technology everyday that changes the way we interact with the world.\nThe impact of newer tech is often less obvious.\nIt is clear that directly cooperating with genocidal regimes like the case of IBM or skirting public safety regulations like Theranos with their ‚Äúone drop of blood‚Äù medical diagnostic machines are both extremely unethical decisions.\nCompare those examples to the common modern case of a recommendation algorithm running on a computer in a distant server farm that chooses what content people see on a website or app.\nAt face value it seems much more benign right?\nAlgorithms must be more objective because they are based on data, and in the case of machine learning algorithms, lots and lots of data at that.\nIt might seem that way at first until you discover that your video recommendation system is curating playlists of prepubescent partially clothed children to drive engagement of pedophiles on your platform.\nThis was a very real problem caused by YouTube for the families who had uploaded their innocent home videos of their kids simply enjoying a pool day.\nThe objectives of the algorithm behind Youtube‚Äôs pedophilia curation was actually quite similar to the that of IBM and Theranos.\nSimilar to IBM‚Äôs CEO Tom Watson and Theranos CEO Elizabeth Holmes, the algorithm strived to optimize it‚Äôs performance metrics by any means necessary.\nWhen the algorithm received positive feedback by seeing the time spent by a user extremely interested in videos of partially clothed children increase, it was reinforced to serve them more of that content.\nWhen Tom Watson saw IBM profits soar when selling services to Nazi Germany, he was positively reinforced to develop more efficient systems to support genocide.\nWhen Elizabeth Holmes saw herself becoming more and more famous by deceiving patients, employees, and investors, she was positively reinforced to keep the act going.\nEach of these actors was caught in a feedback loop that rewarded them for helping the depraved at the expense of the innocent.\nEach actor lacked a moral compass to point them in a better direction for the good of society.\nLastly, each actor did not act alone and had supporting actors that were privy to was going on.\nThey too bear responsibility as they were complicit in the ethical violations of their company.\nA major difference to consider between these actors is that even though algorithms are biased like people, they are not actually people (yet).\nHowever, they still inherit the biases of the people that created and trained them."
  },
  {
    "objectID": "posts/01-algorithms-inherit-bias/index.html#types-of-biases",
    "href": "posts/01-algorithms-inherit-bias/index.html#types-of-biases",
    "title": "Algorithms inherit the prejudices of their creators and society.",
    "section": "Types of biases",
    "text": "Types of biases\nLet‚Äôs define a few different types of biases inherited from people to machine learning algorithms.\n\nHistorical bias\nPeople, processes, and society are inherently biased stemming from the past which effects all datasets.\nExample race bias:\nBlack people have historically been victim to racism by white people and this is reflected in data used by algorithms.\nThe COMPAS algorithm that determines sentencing and bail in the US showed obvious racial bias by disproportionately labeling black Americans as higher risk to re-offend than white Americans by ~20% despite the results showing the predictions should be much closer.\n\nMeasurement bias\nWhen we measure the wrong thing or measure it in the wrong way, models make mistakes.\nExample:\nPredictive models trained on electronic health records determined which factors such as ‚Äúcolonoscopy‚Äù and ‚Äúaccidental injury‚Äù are highly correlated to being diagnosed with a stroke.\nThe real prediction here is that people who are more likely to go to doctors at all are more likely get diagnosed with a stroke because they show up more often in the first place.\nRepresentation bias\nThis one was confusing to me as it seems awfully similar to historical bias.\nThe idea is that models amplifies the existing bias in the dataset they are trained on.\nExample:\nIn society there are some occupations that employ more women than men.\nFor example, there are more women who are nurses and there are more men who are pastors.\nOccupation prediction models not only demonstrated the existing gender discrepancy across occupations but also over amplified the numbers.\nWomen were more likely to be nurses while men more likely to be rappers both at a significantly higher margin than was actually true in the training data.\n\nThat example seems similar to the COMPAS algorithm as both seem to stem from historical bias.\nOne exhibits gender bias and the other racial.\nMaybe the difference lies in how large the amplification of bias between predictions and the training data is."
  },
  {
    "objectID": "posts/01-algorithms-inherit-bias/index.html#responsibility-of-data-engineers",
    "href": "posts/01-algorithms-inherit-bias/index.html#responsibility-of-data-engineers",
    "title": "Algorithms inherit the prejudices of their creators and society.",
    "section": "Responsibility of data engineers",
    "text": "Responsibility of data engineers\nThe consideration of ethics maybe isn‚Äôt for everyone.\nIt is more suited to people with a growth mindset who want to better themselves along with the world around them.\nPeople who spent the time to get through school and work their way into the data science field are definitely capable of logically analyzing complex systems.\nIt is a matter of devoting some of their brain power they spent years building to think about how the model they are training can better serve people and animals outside of their company rather than just the ones within.\nThere are no 100% correct answers when it comes to handling biases ethically.\nHowever, there is value in weighing which approaches are better and which are worse.\nAlthough algorithms are not people, hopefully you are convinced that they do inherit the biases of people.\nIn comparison to traditional hardware tech, algorithms are:\n\ncheap to develop\nmore likely to be implemented without an appeals process\ncapable of scaling quickly\n\nThis makes them more volatile and prone to feedback loops that can be highly detrimental.\nWhat are some counter measures data engineers can take?\n\nMake algorithms more transparent and able to be visualized by non-technical people\nUse more gradual deployment processes with more (diverse) human verification at steps along the way\nThink about what bias is in training data and how to mitigate it such as sometimes choosing to omit some biased factors such as ‚Äògender‚Äô\n\nAsk questions like ‚ÄúHow might future generations be affected by this project?‚Äù or ‚ÄúWhich option will produce the most good and do the least harm?‚Äù\n\nThanks for reading all the way through!\nIf you are curious about reading on about other cases of algorithms gone wrong these were interesting reads:\nDiscrimination in Online Ad Delivery by Latanya Sweeney\nWhat happens when an algorithm cuts your health care"
  },
  {
    "objectID": "posts/04-devlog-day3/index.html",
    "href": "posts/04-devlog-day3/index.html",
    "title": "Devlog Day 3",
    "section": "",
    "text": "Today I learned about GDScript, Godot‚Äôs scripting language.\nIt looks like a blend of Python and Javascript.\nPython because of the indentation and Javascript because of the var keyword used for creating variables.\nI did not have as much time today so I did about 3 of the starter lessons. Most of it was a little too simple but did at least learn some syntax and a few builtin functions.\n# shows a game entity like a character\nshow()\n\n# hides the entity\nhide()\n\n# what you would expect\nrotate(radians:float)\nUsed those to make this turtle draw some rectangles."
  },
  {
    "objectID": "posts/13-devlog-day12/index.html",
    "href": "posts/13-devlog-day12/index.html",
    "title": "Devlog Day 12",
    "section": "",
    "text": ":h tabpage\nTabs in Vim are similar to tabs in other programs however the key difference is that one tab in Vim does not map to one file.\nInstead, a tab in vim can have several buffers within it as well a window panes that show those buffers.\n\nHere are the commands:\n:lcd # set the working dir for the current window\n:tabe {filename} # like edit but open buffer in new tab\n&lt;C-w&gt;T # break the window pane into a new tab\n:tabc # close current tab\n:tabo # close other tabs\n:tabn or gt # next tab\n:tabp or gT # prev tab\n:tabmove {N} # move the Nth tab to beginning\n\n\n\nI also learned about how to open files to buffers quicker rather than typing out full paths.\nI already had discovered the % in command mode which represents the current buffer‚Äôs filepath.\nCombine it into : %:h and press  and the full path to the current buffer is completed for you.\nThis is handy for when you opened Vim at the root project dir but are opening files nested a few folders down.\nIf you already have one open, instead of typing out path from root, you can type the path out relative to the current folder."
  },
  {
    "objectID": "posts/13-devlog-day12/index.html#vim",
    "href": "posts/13-devlog-day12/index.html#vim",
    "title": "Devlog Day 12",
    "section": "",
    "text": ":h tabpage\nTabs in Vim are similar to tabs in other programs however the key difference is that one tab in Vim does not map to one file.\nInstead, a tab in vim can have several buffers within it as well a window panes that show those buffers.\n\nHere are the commands:\n:lcd # set the working dir for the current window\n:tabe {filename} # like edit but open buffer in new tab\n&lt;C-w&gt;T # break the window pane into a new tab\n:tabc # close current tab\n:tabo # close other tabs\n:tabn or gt # next tab\n:tabp or gT # prev tab\n:tabmove {N} # move the Nth tab to beginning\n\n\n\nI also learned about how to open files to buffers quicker rather than typing out full paths.\nI already had discovered the % in command mode which represents the current buffer‚Äôs filepath.\nCombine it into : %:h and press  and the full path to the current buffer is completed for you.\nThis is handy for when you opened Vim at the root project dir but are opening files nested a few folders down.\nIf you already have one open, instead of typing out path from root, you can type the path out relative to the current folder."
  },
  {
    "objectID": "posts/13-devlog-day12/index.html#godot",
    "href": "posts/13-devlog-day12/index.html#godot",
    "title": "Devlog Day 12",
    "section": "Godot",
    "text": "Godot\nAdded a healthbar UI element to the spaceship health pickup scene.\nUse a Control node with ProgressBar node nested.\nGo into the control section of the inspector tab to change colors via theme overrides (or theme if one is made).\nNote though that since these nodes are nested under a parent node that rotates, the bar will also rotate with this ship.\nTo prevent this, use get_node(\"Sprite2D\") or $Sprite2D to access the rotation property of the ship directly.\nI think this might be a problem though if the root scene node for the ship was not an Area2D with collision shape circle.\nIf the shape was anything else, rotating the Sprite without the collision shape would cause a desync in hitboxes relative to the visual sprite.\nCould use the same get_node idea to also rotate the collision shape but maybe there is a cleaner way.\n\nThanks for reading!"
  },
  {
    "objectID": "posts/09-devlog-day8/index.html",
    "href": "posts/09-devlog-day8/index.html",
    "title": "Devlog Day 8",
    "section": "",
    "text": "Some quick notes on Vim:\nBuffers are in memory representations of files (and probably other things)\nYou can put a list of vim commands into a file and source it to run on buffers.\ne.g.¬†:source {script}.vim in command mode\nvim {pattern}*.{extension} opens a bunch of files matching the wildcard into buffers.\n:ls or :argslists the buffers open.:first:nextor:n:prevor:previousor:bnextor:bprev:bfirst:blast` ‚Ä¶ these all iterate through open buffers.\nOn Godot I completed a practice problem basd on the steering algorithm from yesterday.\nI got a little stuck as the algorithm confused me.\nAfter looking through the notes again it became more clear.\nx = input.axis(x) # right or left\ny = input.axis(y) # down or up\n\nmax_speed = 600 # px per frame\nsteering_factor = 10.0 # how jittery should the movement be to a new input\n\ndirection = Vector2D(x,y).normalized() # normalized to keep length less than 1\n\ndesired_velocity = max_speed * direction\n\n# this part is the visual of the vector that connects from the head of the current velocity to the head of the desired_velocity\nsteering_vector =  desired_velocity - velocity # velocity is the previous velocity\n\n# this part confused me. Why add to the velocity? \n# We are adding the steering_vector scaled down by the delta (probably 1/60th of a seond given usual 60 FPS) \n# It is also scaled down by the arbitrary steering_factor we defined above. So as long as the steering_factor is less than 60, there should be some smoothing\n# At 60 I belive it would look the same as never smoothing the velocity as you are adding the difference between the desired_velocity and the current velocity to the current velocity.\n# Cancel out current velocity, you are left with just the desired_velocity\nvelocity += steering_vector * steering_factor * delta\nposition = velocity * delta"
  },
  {
    "objectID": "posts/06-devlog-day5/index.html",
    "href": "posts/06-devlog-day5/index.html",
    "title": "Devlog Day 5",
    "section": "",
    "text": "After watching some ThePrimegean vids, I was inspired to get back into using NeoVim as my primary text editor.\nI had the ‚ÄúPractical Vim‚Äù book lying around so I picked it back up again and started practicing.\nSome Vim notes I took today:\nEx Commands are intended for longer range where as normal commands are more local to a region of the document being edited.\nCapital V is a full linve visual select rather than lower v.\n:t or :co is shortand for copy.\n:m is shorthand for move :$ is end of file.\n:1 is first line\n:'&lt;,'&gt; is current visual selection.\n:{range} normal . runs the prev normal mode command on the range. this builds off that . is the key to repeat the last normal mode command. :%: repeats the last EX command\nBack to GDQuest Godot:\nFrom the godot-vim repo I downloaded the zip and enabled the plugin for Vim bindings in Godot editor.\nSome of the bindings are missing like execute mode and Ctrl+[ to exit insert mode unfortunately.\nIn Godot, position of objects is manipulated by the position attribute of the node.\nYou can add or subtract 2D vectors to change the position relative to the prev position in the _process method of the node.\nVector2D comes with an angle method that gives the angle of the vector probably in relation to to the positive direction of the x axis.\nUsing this angle, you can manipulate the rotation attribute of a node to keep the direction of the object facing the direction of velocity.\nHelpful for sprites like this ship."
  },
  {
    "objectID": "posts/14-devlog-day13/index.html",
    "href": "posts/14-devlog-day13/index.html",
    "title": "Devlog Day 12",
    "section": "",
    "text": "Maybe a year or two ago, I jotted an idea down for a portable workstation down in my digital notebook (Obsidian with Syncthing for the curious).\nIsn‚Äôt that just a laptop you may wonder?\nPretty much except with all of the parts more modular, and I am not talking like Framework laptops.\nI mean, take a mini PC, a portable monitor (with VESA compatibilty), a lightweight VESA mount, a keyboard, a mouse, and optionally although not really, a battery.\nWhy though?\n\nLaptop monitors being tethered to keyboards and touchpads is not great for your shoulders and neck\n\nEach part is swappable. There are many options for each component that goes into it\n\nMini PCs are cheaper than many laptops for equal specs\n\nFun project\n\nI had all the parts sorted, from the Beelink SER5 mini PC to the HHKB Pro classic mech keyboard to the Kensington Trackball mouse along with some shitty cheap LCD monitor.\nI picked out one cheap 1 monitor VESA mount to try which worked great.\nI got a second one that could hold two monitors that needed some effort to mod to fit in my backpack.\nUnfortunately I stripped a few of the bolts so that part is on hold, but the one monitor stand works fine.\nI then neeed to pick out a battery.\nI saw some interesting multi-device battery pack that advertized 250 W, 230 Wh with an AC outlet and a few USB-C ports.\nSeemed a little overkill but would do the job."
  },
  {
    "objectID": "posts/14-devlog-day13/index.html#how-it-started",
    "href": "posts/14-devlog-day13/index.html#how-it-started",
    "title": "Devlog Day 12",
    "section": "",
    "text": "Maybe a year or two ago, I jotted an idea down for a portable workstation down in my digital notebook (Obsidian with Syncthing for the curious).\nIsn‚Äôt that just a laptop you may wonder?\nPretty much except with all of the parts more modular, and I am not talking like Framework laptops.\nI mean, take a mini PC, a portable monitor (with VESA compatibilty), a lightweight VESA mount, a keyboard, a mouse, and optionally although not really, a battery.\nWhy though?\n\nLaptop monitors being tethered to keyboards and touchpads is not great for your shoulders and neck\n\nEach part is swappable. There are many options for each component that goes into it\n\nMini PCs are cheaper than many laptops for equal specs\n\nFun project\n\nI had all the parts sorted, from the Beelink SER5 mini PC to the HHKB Pro classic mech keyboard to the Kensington Trackball mouse along with some shitty cheap LCD monitor.\nI picked out one cheap 1 monitor VESA mount to try which worked great.\nI got a second one that could hold two monitors that needed some effort to mod to fit in my backpack.\nUnfortunately I stripped a few of the bolts so that part is on hold, but the one monitor stand works fine.\nI then neeed to pick out a battery.\nI saw some interesting multi-device battery pack that advertized 250 W, 230 Wh with an AC outlet and a few USB-C ports.\nSeemed a little overkill but would do the job."
  },
  {
    "objectID": "posts/14-devlog-day13/index.html#failure",
    "href": "posts/14-devlog-day13/index.html#failure",
    "title": "Devlog Day 12",
    "section": "Failure",
    "text": "Failure\nI put all the pieces together, testing the battery powering all the devices.\nAfter preparing my USB with a Linux distro (Arch btw), I started going through the installation procedure.\nA few minutes in, the mini PC powered down randomly.\nI noticed the USB-C powering the monitor was significantly chewed up by one of the cats so I replaced it and tried again.\nI got a lot further this time but about 20 minutes in, the PC crashed again.\nI was out of time for some plans with friends but I could not help wondering what was going wrong.\nWhen I got back home I checked the specs of the battery pack on the device.\nFor the AC outlet, which the mini PC power adapter was plugged into, the specs read ‚Äú120V ~0.83 A‚Äù meaning 110 Volts and 0.83 Amps.\nI compared that to the specs on the power adapter which read ‚ÄúAC input, 100-240V 1.5A‚Äù.\nSo I was pretty sure something was off there.\nI have not dealt with electriconics physics in a while so I had to some web searching.\nTurns out, as I relearned, power (Watts) is volts * amps.\nWatts is the unit measurement of power for electrical devices.\nThen Watt hours, or Wh, is the power used (or provided by a battery) in an hour.\nBack to the battery, with our equation power = volts * amps, 110 V * .83 A is 100 Watts.\nThat is a lot less than the advertized 250 W I saw before.\nI went back to the web page and noticed that lower down there was a bullet that read charges a laptop 3 times, 100 W max fast charging.\n100 W max fast charging?\nI realized they must have meant each port was only capable of 100 W max and that 250 Watts was the total power capacity of the battery.\nThe vendor definetly could make the description more clear.\nI setup a return for that battery pack and started looking for a new one."
  },
  {
    "objectID": "posts/14-devlog-day13/index.html#quest-for-the-battery",
    "href": "posts/14-devlog-day13/index.html#quest-for-the-battery",
    "title": "Devlog Day 12",
    "section": "Quest for the battery",
    "text": "Quest for the battery\nI did some math this time around.\nOn the specs of the mini PC, I saw that it takes 19V DC at 3.7A.\nThat is 65 Watts max.\nThe monitor takes 5V 2A or 10 Watts which can be lowered further by dropping the frame rate, resolution, and brightness.\nThe other peripherals like mouse and keyboard would add maybe another 5 watts max.\nAt highest power consumption, 65 + 10 + 5 = 80 watts for the workstation.\nRealistically if I was only programming with limited web browsing on a lightweight distro like Arch with XFCE, this would probably be closer to 50W.\nI estimated that at a coffee shop I would want at least 2 hours of battery power before needing to find an outlet to be reasonable.\nTherefore, I needed 50W * 2h = 100 Wh battery with 19V and &gt;3.7A DC output and &gt;80 Watts max.\nFound a couple that met those specs and included barrel connector adapters to plug into the mini PC DC in.\nThe one I chose ended up specifying mAh (miliAmp hours) rather than Watts so I had to convert.\nLithium Ion batteries have an internal voltage of 3.7V.\nWh = 3.7 * I mAh / 1000.\nThe battery advertized 50000 mAh so 3.7 * 50000 / 1000 = 185 wH.\nWorst case that gives me 2 hours of charge, average case more like 3.5 hours.\nNot too shaby.\nBonus was that it only 2 lbs compared to the other one which was 5+ lbs and much larger.\nUnforseen con of all of this is that none of these batteries are allowed on planes."
  },
  {
    "objectID": "posts/14-devlog-day13/index.html#thing-i-relearned",
    "href": "posts/14-devlog-day13/index.html#thing-i-relearned",
    "title": "Devlog Day 12",
    "section": "Thing I (re)learned",
    "text": "Thing I (re)learned\n\nPower (Watts) = Volts (V) * Current (Amps)\n\nWh = Watt hours = Watts * hours\n\nYou need to make sure that batteries are capable of delivering enough watts for all the devices they are powering (sounds obvious but not relevant in most daily life as a programmer)\n\nLaptop batteries are crazy efficient for their size and safety compatibility\n\nLithium Ion batteries operate at 3.7V\n\nBatteries need to be low Wh and watts to get on planes\n\nBuying a mini PC with a USB-C cable or 12V DC in would have been smarter for battery options\n\nWhen things go wrong, do some research\n\nThis ended up taking me 3 hours today so no update on gamedev or Vim.\nThanks for reading!"
  }
]