[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts üêíüêßüêç",
    "section": "",
    "text": "Devlog Day 1\n\n\n\n\n\n\n\ngamedev\n\n\nvideo games\n\n\ntech\n\n\n\n\nGetting started with 2D Game Development in Godot\n\n\n\n\n\n\nJul 30, 2024\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nAlgorithms inherit the prejudices of their creators and society.\n\n\n\n\n\n\n\nethics\n\n\nai\n\n\ntech\n\n\n\n\nMy notes and thoughts on Deep Learning for Coders: Chapter 3 data ethics\n\n\n\n\n\n\nOct 17, 2023\n\n\nEvan Lesmez\n\n\n\n\n\n\n  \n\n\n\n\nFirst Steps: AI for Animal Advocacy with Fastai üêá\n\n\n\n\n\n\n\nblog\n\n\nfastai\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nEvan Lesmez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01-algorithms-inherit-bias/index.html",
    "href": "posts/01-algorithms-inherit-bias/index.html",
    "title": "Algorithms inherit the prejudices of their creators and society.",
    "section": "",
    "text": "IBM leadership with Adolf Hitler\nIn 1939, under the direction of its president, Thomas Watson, IBM released specialized alphabetizing machines.\nThese machines along with other IBM products played a pivotal role in organizing the deportation of Polish Jews for Nazi Germany.\nThe IBM leadership didn‚Äôt just passively supply; they actively marketed their technology directly to Hitler and his top officials.\nJust a few years prior, IBM‚Äôs CEO, Tom Watson Sr., had even been honored with a ‚ÄúService to the Reich‚Äù medal.\nThe company implemented a punch card system to track the method of execution and the ethnicity of each victim.\nAnd to ensure these machines ran efficiently, IBM staff were often present on-site at concentration camps for operation and maintenance.\nIBM thrived financially and technologically, but at what cost to humanity?\nIBM concentration camp punch-card - Source\nWe are all aware of the horrors of the holocaust and can agree IBM‚Äôs involvement is an egregious example of focusing on technology without care to its impact on society.\nNowadays we are fortunate enough to be in, for the most part, more peaceful times.\nHowever the same ethical questions still arise with development of new technology everyday that changes the way we interact with the world.\nThe impact of newer tech is often less obvious.\nIt is clear that directly cooperating with genocidal regimes like the case of IBM or skirting public safety regulations like Theranos with their ‚Äúone drop of blood‚Äù medical diagnostic machines are both extremely unethical decisions.\nCompare those examples to the common modern case of a recommendation algorithm running on a computer in a distant server farm that chooses what content people see on a website or app.\nAt face value it seems much more benign right?\nAlgorithms must be more objective because they are based on data, and in the case of machine learning algorithms, lots and lots of data at that.\nIt might seem that way at first until you discover that your video recommendation system is curating playlists of prepubescent partially clothed children to drive engagement of pedophiles on your platform.\nThis was a very real problem caused by YouTube for the families who had uploaded their innocent home videos of their kids simply enjoying a pool day.\nThe objectives of the algorithm behind Youtube‚Äôs pedophilia curation was actually quite similar to the that of IBM and Theranos.\nSimilar to IBM‚Äôs CEO Tom Watson and Theranos CEO Elizabeth Holmes, the algorithm strived to optimize it‚Äôs performance metrics by any means necessary.\nWhen the algorithm received positive feedback by seeing the time spent by a user extremely interested in videos of partially clothed children increase, it was reinforced to serve them more of that content.\nWhen Tom Watson saw IBM profits soar when selling services to Nazi Germany, he was positively reinforced to develop more efficient systems to support genocide.\nWhen Elizabeth Holmes saw herself becoming more and more famous by deceiving patients, employees, and investors, she was positively reinforced to keep the act going.\nEach of these actors was caught in a feedback loop that rewarded them for helping the depraved at the expense of the innocent.\nEach actor lacked a moral compass to point them in a better direction for the good of society.\nLastly, each actor did not act alone and had supporting actors that were privy to was going on.\nThey too bear responsibility as they were complicit in the ethical violations of their company.\nA major difference to consider between these actors is that even though algorithms are biased like people, they are not actually people (yet).\nHowever, they still inherit the biases of the people that created and trained them."
  },
  {
    "objectID": "posts/01-algorithms-inherit-bias/index.html#types-of-biases",
    "href": "posts/01-algorithms-inherit-bias/index.html#types-of-biases",
    "title": "Algorithms inherit the prejudices of their creators and society.",
    "section": "Types of biases",
    "text": "Types of biases\nLet‚Äôs define a few different types of biases inherited from people to machine learning algorithms.\n\nHistorical bias\nPeople, processes, and society are inherently biased stemming from the past which effects all datasets.\nExample race bias:\nBlack people have historically been victim to racism by white people and this is reflected in data used by algorithms.\nThe COMPAS algorithm that determines sentencing and bail in the US showed obvious racial bias by disproportionately labeling black Americans as higher risk to re-offend than white Americans by ~20% despite the results showing the predictions should be much closer.\n\nMeasurement bias\nWhen we measure the wrong thing or measure it in the wrong way, models make mistakes.\nExample:\nPredictive models trained on electronic health records determined which factors such as ‚Äúcolonoscopy‚Äù and ‚Äúaccidental injury‚Äù are highly correlated to being diagnosed with a stroke.\nThe real prediction here is that people who are more likely to go to doctors at all are more likely get diagnosed with a stroke because they show up more often in the first place.\nRepresentation bias\nThis one was confusing to me as it seems awfully similar to historical bias.\nThe idea is that models amplifies the existing bias in the dataset they are trained on.\nExample:\nIn society there are some occupations that employ more women than men.\nFor example, there are more women who are nurses and there are more men who are pastors.\nOccupation prediction models not only demonstrated the existing gender discrepancy across occupations but also over amplified the numbers.\nWomen were more likely to be nurses while men more likely to be rappers both at a significantly higher margin than was actually true in the training data.\n\nThat example seems similar to the COMPAS algorithm as both seem to stem from historical bias.\nOne exhibits gender bias and the other racial.\nMaybe the difference lies in how large the amplification of bias between predictions and the training data is."
  },
  {
    "objectID": "posts/01-algorithms-inherit-bias/index.html#responsibility-of-data-engineers",
    "href": "posts/01-algorithms-inherit-bias/index.html#responsibility-of-data-engineers",
    "title": "Algorithms inherit the prejudices of their creators and society.",
    "section": "Responsibility of data engineers",
    "text": "Responsibility of data engineers\nThe consideration of ethics maybe isn‚Äôt for everyone.\nIt is more suited to people with a growth mindset who want to better themselves along with the world around them.\nPeople who spent the time to get through school and work their way into the data science field are definitely capable of logically analyzing complex systems.\nIt is a matter of devoting some of their brain power they spent years building to think about how the model they are training can better serve people and animals outside of their company rather than just the ones within.\nThere are no 100% correct answers when it comes to handling biases ethically.\nHowever, there is value in weighing which approaches are better and which are worse.\nAlthough algorithms are not people, hopefully you are convinced that they do inherit the biases of people.\nIn comparison to traditional hardware tech, algorithms are:\n\ncheap to develop\nmore likely to be implemented without an appeals process\ncapable of scaling quickly\n\nThis makes them more volatile and prone to feedback loops that can be highly detrimental.\nWhat are some counter measures data engineers can take?\n\nMake algorithms more transparent and able to be visualized by non-technical people\nUse more gradual deployment processes with more (diverse) human verification at steps along the way\nThink about what bias is in training data and how to mitigate it such as sometimes choosing to omit some biased factors such as ‚Äògender‚Äô\n\nAsk questions like ‚ÄúHow might future generations be affected by this project?‚Äù or ‚ÄúWhich option will produce the most good and do the least harm?‚Äù\n\nThanks for reading all the way through!\nIf you are curious about reading on about other cases of algorithms gone wrong these were interesting reads:\nDiscrimination in Online Ad Delivery by Latanya Sweeney\nWhat happens when an algorithm cuts your health care"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Evan Lesmez",
    "section": "",
    "text": "This a blog about about my learning journey delving into topics like deep learning/AI, game development, other programming domains, and random ideas. I am a software engineer. My main passions are veganism and animal activism, games, and tech."
  },
  {
    "objectID": "posts/00-first-steps-fastai/index.html",
    "href": "posts/00-first-steps-fastai/index.html",
    "title": "First Steps: AI for Animal Advocacy with Fastai üêá",
    "section": "",
    "text": "Code\nfrom PIL import Image\n\nsmol_forest_guardian = Image.open(\n    \"./DALL¬∑E-digital_art_cute_solarpunk_forest_guardian_robot.png\"\n)\ndisplay(\n    smol_forest_guardian.resize((400, 400)),\n    \"Solar punk forest guardian source: DALLE¬∑2\",\n)\n\n\n\n\n\n'Solar punk forest guardian source: DALLE¬∑2'\nThis charming little forest robot was created using OpenAI‚Äôs DALL¬∑E 2 model, based on my prompt: ‚ÄúDigital art cute solarpunk forest guardian robot‚Äù.\nThis image represents an idea I‚Äôve been interested in for a long time. I‚Äôm not certain where it all started, but I think it goes back to my childhood. That‚Äôs when my aunt introduced me to my first Miyazaki movies: ‚ÄòMy Neighbor Totoro‚Äô, ‚ÄòSpirited Away‚Äô, and ‚ÄòCastle in the Sky‚Äô.\nCode\nbig_guardian = Image.open(\"./castle_in_sky_guardian.jpg\")\nratio = 0.33\nnew_dimens = (round(big_guardian.width * ratio), round(big_guardian.height * ratio))\ndisplay(\n    big_guardian.resize(new_dimens),\n    \"Castle in the Sky Guardian source: https://characterdesignreferences.com/art-of-animation-9/art-of-castle-in-the-sky\",\n)\n\n\n\n\n\n'Castle in the Sky Guardian source: https://characterdesignreferences.com/art-of-animation-9/art-of-castle-in-the-sky'\nMost Hayao Miyazaki fans, I believe, can relate to the sense of awe they feel when watching his films. He has a unique ability to instill a profound respect for nature, capturing its simple beauty and serving as a daily reminder of how much we often take it for granted. His films advocate for the protection of nature from human exploitation and emphasize the importance of reconnecting with the world around us. The Laputian robot from ‚ÄòCastle in the Sky‚Äô, depicted above, stands as a compelling example of this message.\nA few years ago, perhaps inspired by Miyazaki‚Äôs works, I realized my mission: to develop technology that champions the rights of non-human animals and safeguards our shared ecosystems. I envision a future where AI not only respects nature more deeply than humans currently do, but also unravels its secrets that remain undiscovered.\nTo take steps towards this goal, I am embarking on a journey to learn about deep learning, one of the most promising fields within AI. This blog will serve as a record of my progress, where I‚Äôll document my practice and share related ideas, lessons, and questions that arise along the way."
  },
  {
    "objectID": "posts/00-first-steps-fastai/index.html#fast.ai",
    "href": "posts/00-first-steps-fastai/index.html#fast.ai",
    "title": "First Steps: AI for Animal Advocacy with Fastai üêá",
    "section": "fast.ai",
    "text": "fast.ai\nFastai is a vibrant community of deep learning enthusiasts, dedicated to making AI accessible to all. I‚Äôm currently going through their ‚ÄòPractical Deep Learning for Coders‚Äô course, which has been fantastic thus far!\nI‚Äôd highly recommend this course to anyone with even a hint of programming experience who‚Äôs curious about AI. This is particularly true if you‚Äôre in an industry where AI development is still in its infancy - there could be a significant opportunity waiting.\n\nSurprising Discoveries (so far)\nI was astounded by the speed at which I could train and deploy my first model - all within a few weeks of learning.\nTransfer learning is a technique that involves taking a pre-trained model with expert-determined weights and fine-tuning it with your specific data.\nThis strategy allows you to quickly implement a functioning model, without the need to start from scratch each time. As an example, I trained a simple greenhouse/hydroponic plant health classifier using a pre-trained image classifier model based on the ResNet18 architecture. This was a problem a previous company I worked at was trying to solve, so I thought it would be a fun challenge to undertake myself.\nMy trained model is now deployed on this ü§ó Hugging Face space.\nHere‚Äôs a fun fact ü§ì: A GPU isn‚Äôt necessarily required for a deployed model.\n\n\n\nScreenshot of the plant-doc ü§ó space\n\n\nBelow are some snippets from the training of the model, conducted in a Google Colab notebook.\n\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\n\n# ... create a labeled image datablock and visualize\n\nhydro_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128),\n)\ndls = hydro_dblock.dataloaders(path)\ndls.valid.show_batch(max_n=8, nrows=2, figsize=(8, 5))\n\n\n\n\nLabeled datablock\n\n\n\n# ... use a pretrained learner and fine tune\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\nEpoch table\n\n\n\n\nOvercoming Initial Fears in Deep Learning\nBefore diving into the world of deep learning, I was somewhat daunted by the complexity I feared training and deploying a model would entail. I‚Äôm neither a math whiz nor a master coder, but I found many of the initial concepts far more intuitive than I‚Äôd anticipated.\nFor instance, the practice of maintaining a separate training set of data from a validation set (and a test set) seemed quite logical. The training set provides the model with a foundational understanding of correct answers, like labeled images. The validation set then serves as a quiz for your model, checking its comprehension of the patterns it has learned. In the context of an image classifier, the model must guess which label best matches a given image from the validation set, and then evaluate the confidence level of its correctness or error. This process facilitates the model‚Äôs improvement with each ‚Äúepoch‚Äù or training cycle. Additionally, a completely separate test set, kept hidden from the model, can be used by humans to assess the model‚Äôs performance after training is completed.\n\n\n\nSimplified model training through test diagram\n\n\nSeparating a robust validation set (and test set) helps to prevent overfitting the model to images present only in the training set. Overfitting can render models unreliable for new images encountered outside the ‚Äúlab‚Äù setting.\nFor instance, if you‚Äôre building a cat breed classifier and include numerous images of the same orange cat perched on its cat tower in both the training and validation sets, the model might overfit for that particular scenario.\nAnother concept I found intuitive and valuable is the confusion matrix. The confusion matrix helps us visualize which labels the model was ‚Äúconfused‚Äù by and predicted incorrectly during training. For example, as shown below, the model predicted that a few plants were healthy when they were actually wilted, and vice versa.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\nConfusion matrix\n\n\nWe can also plot the top mistakes to visualize the images where the model made incorrect predictions and evaluate the model‚Äôs confidence in its decisions. Being confidently wrong is problematic, but so is being correct with low confidence. Both scenarios suggest areas where the model can improve.\nIn the first case, the model may have ‚Äòlearned‚Äô incorrect patterns from the training data, leading to high confidence in wrong predictions. In the second case, the model‚Äôs lack of confidence, even when correct, could suggest that it‚Äôs struggling to find clear patterns in the data. These are valuable insights that can guide us in improving the model‚Äôs performance.\n\ninterp.plot_top_losses(5)\n\n\n\n\nPlot of top losses\n\n\n\n\nOpportunities for Deep Learning in the Animal and Vegan Advocacy Movement\nThe Animal and Vegan Advocacy (AVA) movement has a multitude of opportunities to leverage deep learning. Just to name a few:\n\nMonitoring wildlife habitats\nIdentifying illegal deforestation\nFlagging illegal fishing vessels\nBuilding vegan education chatbots\nEnforcing farmed animal welfare standards\n\nSome of these areas are already seeing progress. For example, check out this AI4Animals animal welfare issue camera monitoring system developed in the Netherlands.\nOne of the most intriguing projects I‚Äôve come across in this field is the Earth Species Project. Their goal is to decode non-human communication using Natural Language Processing. The potential to understand the ‚Äòsecret languages‚Äô animals use could undoubtedly foster more compassion.\n\n\nObstacles Faced by the Movement\nNon-profit organizations, particularly those advocating for animal rights, often face resource constraints that aren‚Äôt an issue for for-profit industries. Even within the landscape of animal non-profits, farmed animal activism receives only a fraction of the donations that shelters do.\n\n\n\nChart of animals impacted and donations received for animal charities\n\n\nMoreover, non-profits frequently lag behind in technology adoption, making it challenging not only to attract talent like Machine Learning engineers, but also to pursue deep learning-enabled projects that have the potential to make a significant impact.\nLarge animal agriculture enterprises, armed with extensive resources, are using AI to enhance their efficiency, often without considering animal welfare or ecosystem health. Historically, technology has been used to exploit our environment, damaging natural habitats and harming wildlife. If left unchecked, AI could further this trend.\nWe need to empower compassionate individuals and policymakers to better understand AI. This will ensure its use strikes a healthier balance between technological advancement and nature, rather than exacerbating existing problems.\nThank you for reading, and stay tuned for more posts in the future!\nThis blog was built with Quarto and Jupyter, allowing me to embed fun, interactive code generated blocks like the one below.\nTry hovering over it.\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n\nimport numpy as np\n\n# Helix equation\nt = np.linspace(0, 20, 100)\nx, y, z = np.cos(t), np.sin(t), t\n\nfig = go.Figure(\n    data=[\n        go.Scatter3d(\n            x=x,\n            y=y,\n            z=z,\n            mode=\"markers\",\n            marker=dict(size=12, color=z, colorscale=\"spectral\", opacity=0.8),\n        )\n    ]\n)\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0), width=640, height=640)\nfig.show()"
  },
  {
    "objectID": "posts/02-devlog-day1/index.html",
    "href": "posts/02-devlog-day1/index.html",
    "title": "Devlog Day 1",
    "section": "",
    "text": "Disclaimer: Today was not actually my first day of game development nor Godot.\nI actually went to nerdy chess and ‚Äúcode‚Äù camp back in elementary school and built a simple 2d shooter game.\nA several years ago I tinkered with Unity for about a month but lost interest as work at the startup I was with picked up a lot.\nTwo years ago I discovered Godot and went through the getting started docs.\nI built a simple 2D dodge the creeps game.\n\n\n\n\nYesterday I rebuilt it following along with a GDQuest tutorial and that brings us to today.\nI am starting the Learn 2D Gamedev Godot 4 course.\nWhy am I getting back into gamedev and why am I writing this down?\nAs to why gamedev:\nI have always loved games since I was a kid like Super Smash Bros Melee, Sly Cooper, FIFA, Uncharted 2, CoD Modern Warfare 2, and League of Legends.\nI always thought it would be fun to try making one of the several game ideas I have jotted down in my notebooks over the years.\nI now know how to code decently well.\nI think games are a valuable way to spread ideas, especially on issues I care about like veganism, animal activism, and conservation.\nWhy am I writing this down?\nMostly to keep myself accountable.\nAs echoed in both the fastai and GDQuest curriculum, social pressure is inevitablely motivating for every single person wether we admit it or not.\nAs I don‚Äôt have a group of study partners nearby to work on either of those, I am resorting to posting a daily blog that I will share on social media.\nI‚Äôll try to keep these short and to the point in 250ish words.\nWhat I learned so far on gamdev and Godot:\n* pixel art isn‚Äôt as easy as it looks\n* vector art is great to start with for beginners making their earliest games\n* signals are a convenient design pattern to broadcast events from different game objects to each other to determine what should happen next\n* rendering animations for some basic sprites is way simpler than I thought it would be (at least in Godot)\n* music and sounds add a lot to how a game feels\nFor tomorrow I want to continue with the course and hopefully have finished answering the fastai end of chapter questions I have put off for a while."
  }
]