[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Evan Lesmez",
    "section": "",
    "text": "This a blog about about Ev’s deep learning journey and random ideas 🐒"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "🙃",
    "section": "",
    "text": "First Steps: AI for Animal Advocacy with Fastai 🐇\n\n\n\n\n\n\n\nblog\n\n\nfastai\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nEvan Lesmez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-28-first-post-fastai/index.html",
    "href": "posts/2023-04-28-first-post-fastai/index.html",
    "title": "First Steps: AI for Animal Advocacy with Fastai 🐇",
    "section": "",
    "text": "Code\nfrom PIL import Image\n\nsmol_forest_guardian = Image.open(\n    \"./DALL·E-digital_art_cute_solarpunk_forest_guardian_robot.png\"\n)\ndisplay(\n    smol_forest_guardian.resize((400, 400)),\n    \"Solar punk forest guardian source: DALLE·2\",\n)\n\n\n\n\n\n'Solar punk forest guardian source: DALLE·2'\nThis charming little forest robot was created using OpenAI’s DALL·E 2 model, based on my prompt: “Digital art cute solarpunk forest guardian robot”.\nThis image represents an idea I’ve been interested in for a long time. I’m not certain where it all started, but I think it goes back to my childhood. That’s when my aunt introduced me to my first Miyazaki movies: ‘My Neighbor Totoro’, ‘Spirited Away’, and ‘Castle in the Sky’.\nCode\nbig_guardian = Image.open(\"./castle_in_sky_guardian.jpg\")\nratio = 0.33\nnew_dimens = (round(big_guardian.width * ratio), round(big_guardian.height * ratio))\ndisplay(\n    big_guardian.resize(new_dimens),\n    \"Castle in the Sky Guardian source: https://characterdesignreferences.com/art-of-animation-9/art-of-castle-in-the-sky\",\n)\n\n\n\n\n\n'Castle in the Sky Guardian source: https://characterdesignreferences.com/art-of-animation-9/art-of-castle-in-the-sky'\nMost Hayao Miyazaki fans, I believe, can relate to the sense of awe they feel when watching his films. He has a unique ability to instill a profound respect for nature, capturing its simple beauty and serving as a daily reminder of how much we often take it for granted. His films advocate for the protection of nature from human exploitation and emphasize the importance of reconnecting with the world around us. The Laputian robot from ‘Castle in the Sky’, depicted above, stands as a compelling example of this message.\nA few years ago, perhaps inspired by Miyazaki’s works, I realized my mission: to develop technology that champions the rights of non-human animals and safeguards our shared ecosystems. I envision a future where AI not only respects nature more deeply than humans currently do, but also unravels its secrets that remain undiscovered.\nTo take steps towards this goal, I am embarking on a journey to learn about deep learning, one of the most promising fields within AI. This blog will serve as a record of my progress, where I’ll document my practice and share related ideas, lessons, and questions that arise along the way."
  },
  {
    "objectID": "posts/2023-04-28-first-post-fastai/index.html#fast.ai",
    "href": "posts/2023-04-28-first-post-fastai/index.html#fast.ai",
    "title": "First Steps: AI for Animal Advocacy with Fastai 🐇",
    "section": "fast.ai",
    "text": "fast.ai\nFastai is a vibrant community of deep learning enthusiasts, dedicated to making AI accessible to all. I’m currently going through their ‘Practical Deep Learning for Coders’ course, which has been fantastic thus far!\nI’d highly recommend this course to anyone with even a hint of programming experience who’s curious about AI. This is particularly true if you’re in an industry where AI development is still in its infancy - there could be a significant opportunity waiting.\n\nSurprising Discoveries (so far)\nI was astounded by the speed at which I could train and deploy my first model - all within a few weeks of learning.\nTransfer learning is a technique that involves taking a pre-trained model with expert-determined weights and fine-tuning it with your specific data.\nThis strategy allows you to quickly implement a functioning model, without the need to start from scratch each time. As an example, I trained a simple greenhouse/hydroponic plant health classifier using a pre-trained image classifier model based on the ResNet18 architecture. This was a problem a previous company I worked at was trying to solve, so I thought it would be a fun challenge to undertake myself.\nMy trained model is now deployed on this 🤗 Hugging Face space.\nHere’s a fun fact 🤓: A GPU isn’t necessarily required for a deployed model.\n\n\n\nScreenshot of the plant-doc 🤗 space\n\n\nBelow are some snippets from the training of the model, conducted in a Google Colab notebook.\n\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\n\n# ... create a labeled image datablock and visualize\n\nhydro_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128),\n)\ndls = hydro_dblock.dataloaders(path)\ndls.valid.show_batch(max_n=8, nrows=2, figsize=(8, 5))\n\n\n\n\nLabeled datablock\n\n\n\n# ... use a pretrained learner and fine tune\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\nEpoch table\n\n\n\n\nOvercoming Initial Fears in Deep Learning\nBefore diving into the world of deep learning, I was somewhat daunted by the complexity I feared training and deploying a model would entail. I’m neither a math whiz nor a master coder, but I found many of the initial concepts far more intuitive than I’d anticipated.\nFor instance, the practice of maintaining a separate training set of data from a validation set (and a test set) seemed quite logical. The training set provides the model with a foundational understanding of correct answers, like labeled images. The validation set then serves as a quiz for your model, checking its comprehension of the patterns it has learned. In the context of an image classifier, the model must guess which label best matches a given image from the validation set, and then evaluate the confidence level of its correctness or error. This process facilitates the model’s improvement with each “epoch” or training cycle. Additionally, a completely separate test set, kept hidden from the model, can be used by humans to assess the model’s performance after training is completed.\n\n\n\nSimplified model training through test diagram\n\n\nSeparating a robust validation set (and test set) helps to prevent overfitting the model to images present only in the training set. Overfitting can render models unreliable for new images encountered outside the “lab” setting.\nFor instance, if you’re building a cat breed classifier and include numerous images of the same orange cat perched on its cat tower in both the test and validation sets, the model might overfit for that particular scenario.\nAnother concept I found intuitive and valuable is the confusion matrix. The confusion matrix helps us visualize which labels the model was “confused” by and predicted incorrectly during training. For example, as shown below, the model predicted that a few plants were healthy when they were actually wilted, and vice versa.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\nConfusion matrix\n\n\nWe can also plot the top mistakes to visualize the images where the model made incorrect predictions and evaluate the model’s confidence in its decisions. Being confidently wrong is problematic, but so is being correct with low confidence. Both scenarios suggest areas where the model can improve.\nIn the first case, the model may have ‘learned’ incorrect patterns from the training data, leading to high confidence in wrong predictions. In the second case, the model’s lack of confidence, even when correct, could suggest that it’s struggling to find clear patterns in the data. These are valuable insights that can guide us in improving the model’s performance.\n\ninterp.plot_top_losses(5)\n\n\n\n\nPlot of top losses\n\n\n\n\nOpportunities for Deep Learning in the Animal and Vegan Advocacy Movement\nThe Animal and Vegan Advocacy (AVA) movement has a multitude of opportunities to leverage deep learning. Just to name a few:\n\nMonitoring wildlife habitats\nIdentifying illegal deforestation\nFlagging illegal fishing vessels\nBuilding vegan education chatbots\nEnforcing farmed animal welfare standards\n\nSome of these areas are already seeing progress. For example, check out this AI4Animals animal welfare issue camera monitoring system developed in the Netherlands.\nOne of the most intriguing projects I’ve come across in this field is the Earth Species Project. Their goal is to decode non-human communication using Natural Language Processing. The potential to understand the ‘secret languages’ animals use could undoubtedly foster more compassion.\n\n\nObstacles Faced by the Movement\nNon-profit organizations, particularly those advocating for animal rights, often face resource constraints that aren’t an issue for for-profit industries. Even within the landscape of animal non-profits, farmed animal activism receives only a fraction of the donations that shelters do.\n\n\n\nChart of animals impacted and donations received for animal charities\n\n\nMoreover, non-profits frequently lag behind in technology adoption, making it challenging not only to attract talent like Machine Learning engineers, but also to pursue deep learning-enabled projects that have the potential to make a significant impact.\nLarge animal agriculture enterprises, armed with extensive resources, are using AI to enhance their efficiency, often without considering animal welfare or ecosystem health. Historically, technology has been used to exploit our environment, damaging natural habitats and harming wildlife. If left unchecked, AI could further this trend.\nWe need to empower compassionate individuals and policymakers to better understand AI. This will ensure its use strikes a healthier balance between technological advancement and nature, rather than exacerbating existing problems.\nThank you for reading, and stay tuned for more posts in the future!\nThis blog was built with Quarto and Jupyter, allowing me to embed fun, interactive code generated blocks like the one below.\nTry hovering over it.\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n\nimport numpy as np\n\n# Helix equation\nt = np.linspace(0, 20, 100)\nx, y, z = np.cos(t), np.sin(t), t\n\nfig = go.Figure(\n    data=[\n        go.Scatter3d(\n            x=x,\n            y=y,\n            z=z,\n            mode=\"markers\",\n            marker=dict(size=12, color=z, colorscale=\"spectral\", opacity=0.8),\n        )\n    ]\n)\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0), width=640, height=640)\nfig.show()"
  }
]