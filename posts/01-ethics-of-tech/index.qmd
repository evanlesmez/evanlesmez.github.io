---
title: "Algorithms inherit the prejudices of their creators and society."
description: "My notes and thoughts on Deep Learning for Coders: Chapter 3 data ethics"
author: "Evan Lesmez"
date: "09/08/2023"
draft: true
categories:
  - ethics
  - ai
  - tech
---

![IBM CEO Tom Watson Sr. with Adolf](ibm-ceo-tom-watson-sr-with-adolf.png)  
**IBM leadership with Adolf Hitler**

In 1939, IBM's president Thomas Watson approved the release of the company's specialized alphabetizing machines to help organize the deportation of Polish Jews for Nazi Germany.  
The IBM executives strategically marketed directly to Hitler and his leadership team to setup the deal.  
A few years earlier, IBM CEO Tom Watson Sr. was awarded a "Service to the Reich" medal.  
IBM setup a punchard system that tracked the way each person was killed and which ethnic group they belonged to.  
These machines required training and maintenance so IBM employees would regularly show up onsite at concentration camps to operate and repair them. 
IBM was thriving financially and technologically, but at what cost to humanity?  

 
![IBM concentration camp punchard](ibm-concentration-camp-punchcard.jpeg)  
*IBM concentration camp punchcard*  
[Source](https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb)  


We are all aware of the horrors of the holocaust and can all agree this is an eggrigious example of focusing on techonology without care to its impact on society.  
Nowadays we are fortunate enough to be in more peacefull times (for the most part), however the same ethical questions arise with development of new technology everyday that changes the way we interact with the world.  
The impact of newer tech is often less obvious.  
It is clear that directly cooperating with genocidal groups like the case of IBM or skirting public safety regulations like Theranos with their "one drop of blood" medical diagnotic machines are extermely unethical decisions.  
Compare those examples to the common modern case of some reccomendation algorithm running on a computer in a server farm that chooses what content people see on a website or app.   
At face value it seems much more benign right?
Algorithms must be objective because they are based on data, and in the case of machine learning algorithms, lots and lots of data at that.

It might seem that way at first until you discover that your video recommendation system is curating playlists of prepubescent partially clothed children to drive engagement of pedohiles on your platform.   
[This was a very real problem caused by YouTube for the familes who had uploaded their innocent home videos of their kids simply enjoying a pool day](https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html).  
The objectives of the algorithm behind Youtube's pedophilia curation was actually quite similar to the that of IBM and Theranos.  
Similar to IBM's CEO Tom Watson and Theranos CEO Elizabeth Holmes, the algorithm strived to optimize it's performance metrics by any means necessary.  

When the algorithm received positive feedback by seeing the time spent by a user *exteremely* interested in videos of partially clothed children increase, it was reinforced to serve them more of that content.  
When Tom Watson saw IBM profits soar when selling services to Nazi Germany, he was positively reinforced to develop more efficient systems to support genocide.  
When Elizabeth Holmes saw herself becoming more and more famous by deveiving patients, employees, and investors, she was positively reinforced to keep the act going.  

Each of these agents was caught in a feedback loop that rewarded them for helping the depraved at the expense of the innocent.  
Each agent lacked a moral compass to point them in a better direction for the good of society. 
Lastly, each agent did not act alone and had supporting actors that were privy to was really going on.  
They too bear responsibility as they were complicit in the ethical violations of their company.  

A major difference to consider between these agents is that even though algorithms are biased like people, they are not *actually* people (yet).  
However, they still inherit the biases of the people that created and trained them.  

## Types of biases
Let's define a few different types of biases inherited from people to machine learning algorithms.  
[](image of bias in ML)

**Historical bias**:  
People, processes, and society are inherently biased stemming from the past which effects all datasets.  

Example *race bias* - Black people have historically, been victim to racism by white people and this is reflected in data used by algorithms. The COMPAS algorithm that determines sentencing and bail in the US showed obvious racial bias by disproportianetly labeling black Americans as higher risk to re-offend than white Americans by ~20% despite the results showing the predictions should be much more even.  
[](img of table)

**Measurement bias**:  
When we measure the wrong thing or measure it in the wrong way, models make mistakes.  

Example - Predictive models trained on electronic health rercords determined what factors like factors such as "colonoscopy" and "accidental injury" are highly correlated to being diagnosed with a stroke.  
The real prediction here is that people who are more likely to go to doctors *at all* are more likely get diagnosed with a stroke because they show up more often in the first place.  

**Representation bias**:
This one was confusing to me as it seems offly similar to historical bias.  
The idea is that models amplifies the existing bias in the dataset they are trained on.  

Example - In society there are some occupations that employ more women than men. For example, there are more women who are nurses and there are more men who are pastors. Occupation prediction models not only showed the existing gender discrepancy across occupations but also over amplifeid the numbers.  
Women were more likely to be nurses while men more likely to be rappers both at a significantly higher margin than was actually true in the training data.  
[](chart image of women icarrer)

That example seems offly similar to amplyifying the racial bias exhibited by the COMPAS algorithm however I belive the difference lies in how closely the model predicts outcomes true to the training data.  

## Responsibility of data engineers
There are no 100% correct answers when it comes to handling biases ethically.  
However there is value in thinking through which approaches are better and which are worse.  
Although algorithms are not people, hopefully you are convinced that they do inherit the biases of people.  
In comparison to traditional hardware tech, algorithms are: 
- cheap to develop 
- more likely to be implemented without an appeals process 
- capable of scaling quickly


Her eCounter measures:
- Make algorithms more transparent and able to be visualized by non-technical people
- Use more gradual deployment processes


This consideration of ethics maybe isn't for everyone. 
Growth mindset
that the you of today should strive be better than the you of yesterday.  
It is our responsibility to ensure algorithms make the 
Links for the curious

## Extra links 
- Latanya Sweeney
    - google
    - racism
- healthcare verge